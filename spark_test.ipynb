{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://medium.com/@ashish1512/how-to-setup-apache-spark-pyspark-on-jupyter-ipython-notebook-3330543ab307\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"first app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df = pd.read_csv('data/btc_tweets_1418244_unique.csv', \n",
    "                       encoding='utf-8',\n",
    "                       header=0, \n",
    "                       engine='python',\n",
    "                       error_bad_lines=False,\n",
    "                       usecols=[0,1,2,3,4,5])\n",
    "PS_tweet_df = sql.createDataFrame(tweet_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ps_tw_df = (sc.read.format(\"csv\").options(header=\"true\").load(\"data/btc_tweets_1418244_unique.csv\"))\n",
    "# https://datascience.stackexchange.com/questions/13123/import-csv-file-contents-into-pyspark-dataframes\n",
    "ps_tw_df = sql.read.format(\"com.databricks.spark.csv\").options(header=\"true\", inferschema='true').load(\"data/btc_tweets_1418244_unique.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- permalink_uid: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_tw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+--------+--------------------+--------------------+------------------+\n",
      "|date|              id|    text|           permalink|       permalink_uid|          username|\n",
      "+----+----------------+--------+--------------------+--------------------+------------------+\n",
      "|   0|12/25/2017 11:00|9.45E+17|#Bitcoin Is Alrea...|https://twitter.c...|945323495235481600|\n",
      "|   1|12/25/2017 11:00|9.45E+17|bitcoin $ BTCUSD ...|https://twitter.c...|945323493079617536|\n",
      "|   2|12/25/2017 11:00|9.45E+17|Tire suas dúvidas...|https://twitter.c...|945323491817082881|\n",
      "+----+----------------+--------+--------------------+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_tw_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ps_tw_df = ps_tw_df.withColumnRenamed('date', 'index') #setting column names of Twitter dataset\n",
    "ps_tw_df = ps_tw_df.withColumnRenamed('id', 'date_time') #setting column names of Twitter dataset\n",
    "ps_tw_df = ps_tw_df.withColumnRenamed('text', 'id') #setting column names of Twitter dataset\n",
    "ps_tw_df = ps_tw_df.withColumnRenamed('permalink', 'text') #setting column names of Twitter dataset\n",
    "ps_tw_df = ps_tw_df.withColumnRenamed('permalink_uid', 'permalink') #setting column names of Twitter dataset\n",
    "ps_tw_df = ps_tw_df.withColumnRenamed('username', 'permalink_uid') #setting column names of Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+--------+--------------------+--------------------+------------------+\n",
      "|index|       date_time|      id|                text|           permalink|     permalink_uid|\n",
      "+-----+----------------+--------+--------------------+--------------------+------------------+\n",
      "|    0|12/25/2017 11:00|9.45E+17|#Bitcoin Is Alrea...|https://twitter.c...|945323495235481600|\n",
      "|    1|12/25/2017 11:00|9.45E+17|bitcoin $ BTCUSD ...|https://twitter.c...|945323493079617536|\n",
      "|    2|12/25/2017 11:00|9.45E+17|Tire suas dúvidas...|https://twitter.c...|945323491817082881|\n",
      "+-----+----------------+--------+--------------------+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_tw_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 1\n",
      "this: 1\n",
      "is: 1\n",
      "test!: 1\n",
      "spark: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculating words count\n",
    "# https://medium.com/@ashok.tankala/run-your-first-spark-program-using-pyspark-and-jupyter-notebook-3b1281765169\n",
    "text_file = sc.textFile('data/spark-test.txt')\n",
    "counts = text_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Printing each word with its respective count\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))\n",
    "# Stopping Spark Context\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
